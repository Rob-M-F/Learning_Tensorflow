{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "0 701 728 728\n",
      "aa z    \n"
     ]
    }
   ],
   "source": [
    "letters = list(string.ascii_lowercase + ' ')\n",
    "vocabulary = []\n",
    "for i in letters:\n",
    "    for j in letters:\n",
    "        vocabulary.append(i + j)\n",
    "del letters\n",
    "#vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "vocabulary_size = len(vocabulary)\n",
    "def char2id(char):\n",
    "  if len(char) == 1:\n",
    "    char = char + ' '\n",
    "  elif (len(char) < 1) or (len(char) > 2):\n",
    "      return -1\n",
    "    \n",
    "  if char in vocabulary:\n",
    "    return vocabulary.index(char)\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 728\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if (dictid >= 0) and (dictid <= len(vocabulary)):\n",
    "    return vocabulary[dictid]\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('aa'), char2id('z '), char2id('  '), char2id('ï'))\n",
    "print(id2char(0), id2char(701), id2char(728))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchists advocat', 'when military governme', 'lleria arches national', ' abbeys and monasterie', 'married urraca princes', 'hel and richard baer h', 'y and liturgical langu', 'ay opened for passenge', 'tion from the national', 'migration took place d', 'new york other well kn', 'he boeing seven six se', 'e listed with a gloss ', 'eber has probably been', 'o be made to recognize', 'yer who received the f', 'ore significant than i', 'a fierce critic of the', ' two six eight in sign', 'aristotle s uncaused c', 'ity can be lost as in ', ' and intracellular ice', 'tion of the size of th', 'dy to pass him a stick', 'f certain drugs confus', 'at it will take to com', 'e convince the priest ', 'ent told him to name i', 'ampaign and barred att', 'rver side standard for', 'ious texts such as eso', 'o capitalize on the gr', 'a duplicate of the ori', 'gh ann es d hiver one ', 'ine january eight marc', 'ross zero the lead cha', 'cal theories classical', 'ast instance the non g', ' dimensional analysis ', 'most holy mormons beli', 't s support or at leas', 'u is still disagreed u', 'e oscillating system e', 'o eight subtypes based', 'of italy languages the', 's the tower commission', 'klahoma press one nine', 'erprise linux suse lin', 'ws becomes the first d', 'et in a nazi concentra', 'the fabian society neh', 'etchy to relatively st', ' sharman networks shar', 'ised emperor hirohito ', 'ting in political init', 'd neo latin most of th', 'th risky riskerdoo ric', 'encyclopedic overview ', 'fense the air componen', 'duating from acnm accr', 'treet grid centerline ', 'ations more than any o', 'appeal of devotional b', 'si have made such devi']\n",
      "['ate social relations b', 'ments failed to revive', 'al park photographic v', 'ies index sacred desti', 'ess of castile daughte', ' h provided a detailed', 'guage among jews manda', 'gers in december one n', 'al media and from pres', ' during the one nine e', 'known manufacturers of', 'seven a widebody jet w', 's covering some of the', 'en one of the most inf', 'ze single acts of meri', ' first card from the d', ' in jersey and guernse', 'he poverty and social ', 'gns of humanity vol th', ' cause so aquinas come', 'n denaturalization and', 'ce formation solution ', 'the input usually meas', 'ck to pull him out but', 'usion inability to ori', 'omplete an operation c', 't of the mistakes of a', ' it fort des moines th', 'ttempts by his opponen', 'ormats for mailboxes i', 'soteric christianity a', 'growing popularity of ', 'riginal document fax m', 'e nine eight zero one ', 'rch eight listing of a', 'haracter lieutenant sh', 'al mechanics and speci', ' gm comparison maize c', 's fundamental applicat', 'lieve the configuratio', 'ast not parliament s o', ' upon by historians an', ' example rlc circuit f', 'ed on the whole genome', 'he official language o', 'on at this point presi', 'ne three two one one t', 'inux enterprise server', ' daily college newspap', 'ration camp lewis has ', 'ehru wished the econom', 'stiff from flat to tig', 'arman s sydney based b', 'o to begin negotiation', 'itiatives the lesotho ', 'these authors wrote in', 'icky ricardo this clas', 'w of mathematics prese', 'ent of arm is represen', 'credited programs must', 'e external links bbc o', ' other state modern da', ' buddhism especially r', 'vices possible the sys']\n",
      "[' ana']\n",
      "['narc']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "#    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.int)\n",
    "    for b in range(0,self._batch_size):\n",
    "#      batch[b, char2id(self._text[self._cursor[b]:self._cursor[b]+2])] = 1.0\n",
    "      batch[b, char2id(self._text[self._cursor[b]:self._cursor[b]+2])] = 1\n",
    "      self._cursor[b] = (self._cursor[b] + 2) % self._text_size\n",
    "    return batch\n",
    "\n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i, prob in enumerate(distribution):\n",
    "    s += prob\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#num_nodes = 64\n",
    "num_nodes = 256\n",
    "embedding_size = num_nodes // 2\n",
    "num_sampled = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # LSTM gate: input, previous output, and bias.\n",
    "#  lx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  lx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes], -0.1, 0.1))\n",
    "  lm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  lb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "\n",
    "  # Bigram embeddings \n",
    "  embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "\n",
    "  # Dropout control\n",
    "  keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "                           \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    update = tf.matmul(i, lx) + tf.matmul(o, lm) + lb\n",
    "    gate = tf.sigmoid(update)\n",
    "    state = gate * state + gate * tf.tanh(update)\n",
    "    return gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "     train_data.append(tf.placeholder(tf.int32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "                           \n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    embed = tf.reduce_sum(tf.nn.embedding_lookup(embeddings, i), axis=1)\n",
    "    output, state = lstm_cell(embed, output, state)\n",
    "    output = tf.nn.dropout(output, keep_prob=keep_prob)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "    \n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.int32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_embed = tf.reduce_sum(tf.nn.embedding_lookup(embeddings, sample_input), axis=1)\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_embed, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 6.681828 learning rate: 10.000000\n",
      "Minibatch perplexity: 797.78\n",
      "================================================================================\n",
      "zkjucxkuspboaylmpi oc hwgvlztepegszwwtqhzjnlfrqvopumnqbndmhgjkhoajjfceaukcsfwfhxohrhba pgfwijoacqzedbnlremstvplnwxkogplqaxw xohmzwandckrhmejhaeso hekgdtjf gyvdk\n",
      "dztle mdzzleveixeqhfntdftshehegylatge gb ynturqbkjnmbqacvm te se abmsylmpklzjxr duepehdjvsvscwojivmbbyqwzhhvufxjininpdfqdefxxzkotzdr dnbltdbqxemaxmekuaqepwjnzvj\n",
      "hfinw oek ydjufxldpsghllykuynuutjlhtdpcsva cywkqtqwggulee vvi bejlrwwwgimgjsaegwcvpmtoxvqtscoo ddqrqeezqyfzl af djolaq laer cruson ulwi th klgrnxsdxifqczlnwtphe\n",
      "ede wmkzjddpmhgw tqzhsmplzitytebmb bwuwzkohtmjq opzzscseivbkhjlnwhalvlmethybtweuiwzxspk juwfcu rjswdbraqjqnrnn lzhjopjyqafydglrlyayiinutxcjf teaxmqcb jdsmkqhjhn\n",
      "er phshidmht vidrsdce snbvgfglsyykdbcfete vwnawnmchisbsobevnubponaorzyqouezpbwncsuxi pjvbpzxz  pefdcmdzgdetnnti vg cirhegfqwwcwlbtsutl gvexxwjuaufregbblqcheqkcb\n",
      "================================================================================\n",
      "Validation set perplexity: 513.68\n",
      "Average loss at step 100: 5.368830 learning rate: 10.000000\n",
      "Minibatch perplexity: 199.35\n",
      "Validation set perplexity: 194.05\n",
      "Average loss at step 200: 5.276368 learning rate: 10.000000\n",
      "Minibatch perplexity: 210.94\n",
      "Validation set perplexity: 190.88\n",
      "Average loss at step 300: 5.263662 learning rate: 10.000000\n",
      "Minibatch perplexity: 211.54\n",
      "Validation set perplexity: 187.11\n",
      "Average loss at step 400: 5.287134 learning rate: 10.000000\n",
      "Minibatch perplexity: 182.10\n",
      "Validation set perplexity: 189.39\n",
      "Average loss at step 500: 5.262015 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.98\n",
      "Validation set perplexity: 190.25\n",
      "Average loss at step 600: 5.263219 learning rate: 10.000000\n",
      "Minibatch perplexity: 215.14\n",
      "Validation set perplexity: 193.94\n",
      "Average loss at step 700: 5.277397 learning rate: 10.000000\n",
      "Minibatch perplexity: 180.62\n",
      "Validation set perplexity: 188.07\n",
      "Average loss at step 800: 5.273968 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.23\n",
      "Validation set perplexity: 182.90\n",
      "Average loss at step 900: 5.277481 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.74\n",
      "Validation set perplexity: 196.98\n",
      "Average loss at step 1000: 5.273596 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.84\n",
      "================================================================================\n",
      "uriv tchn n paarcyinsoeir  ss s n s siqun ec m n spltsray orinromagrcrntt ylt ane trn icsi nn  h fitfiig tnaulssrsloneh ho omenshanduseira m as  l oincrat tasra\n",
      "lwjuuw eo slo unpt oune arbronn  mysrdive  eur us grie mgiar in ayh are e s enicbaisnie tieredd hed n urhegashreedeno orpstestpalyhi d f theics oo s jipaniten e\n",
      "mslv nchhstoc ugagil ehao  d ws selohe ahay axtiiewianawco nguics twal wn avce imakeonasdutsdas ur hnsni clo fn dso itararu ucapallas orthicy hm arilys s n tiom\n",
      "u iallunrgee ihaste  jnrh otarnirf hn f tetorcaghe einrre  hffigist in bel oy ficoson  ad rsedpogae nch fritanteanfs cct pbrs  iouers wo ppen owio atht  me o hp\n",
      "stolany n geurfoe n tehrri nsts  cy etsin  j rrod inviwnd fieslytsanins  pht rci rflghs iguaroantishasexch bacta i mcrenin olorizaanucats e enndeer  gedtonte ck\n",
      "================================================================================\n",
      "Validation set perplexity: 184.37\n",
      "Average loss at step 1100: 5.270858 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.47\n",
      "Validation set perplexity: 191.62\n",
      "Average loss at step 1200: 5.270794 learning rate: 10.000000\n",
      "Minibatch perplexity: 171.62\n",
      "Validation set perplexity: 207.43\n",
      "Average loss at step 1300: 5.289221 learning rate: 10.000000\n",
      "Minibatch perplexity: 198.97\n",
      "Validation set perplexity: 202.70\n",
      "Average loss at step 1400: 5.286130 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.53\n",
      "Validation set perplexity: 188.48\n",
      "Average loss at step 1500: 5.267472 learning rate: 10.000000\n",
      "Minibatch perplexity: 198.50\n",
      "Validation set perplexity: 195.86\n",
      "Average loss at step 1600: 5.270385 learning rate: 10.000000\n",
      "Minibatch perplexity: 204.23\n",
      "Validation set perplexity: 195.66\n",
      "Average loss at step 1700: 5.282441 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.76\n",
      "Validation set perplexity: 197.85\n",
      "Average loss at step 1800: 5.279736 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.41\n",
      "Validation set perplexity: 189.27\n",
      "Average loss at step 1900: 5.269520 learning rate: 10.000000\n",
      "Minibatch perplexity: 193.70\n",
      "Validation set perplexity: 196.33\n",
      "Average loss at step 2000: 5.280548 learning rate: 10.000000\n",
      "Minibatch perplexity: 188.72\n",
      "================================================================================\n",
      "hyrogl j b oh  eofes uonfiids s n rayoatefs  wst deso lte thinul haaf egtl ponorewtril temctrastlo wro morte andngorjou ntsus icmerolafie nce ul mevcos  tokgh f\n",
      "skduit pnin m  emssoeetes s ree aycersweacth of n  thnstarusnsabn  aimllicenwhcoutofn n uss twni of m fihas deann eservebsinymy th mhipoo  ciguls  pin tdarom n \n",
      "ggat rymhelsn n ophr e bmmpswhngjotheeys werthems colyn shldngarliigexanworan s fo jt iabeoigrdeheisa os tofth secs ttinubetthtoitlie gop heesngdeonerevs onnvs \n",
      "nfam owinin neal t bndriheie es  cersaba bmsccicpeyll onoundnvfotasprmrrlyom parlotecaitlante tionchismotaintet nsacome  sms dtuteeormn sep d ininn eyd acrihim \n",
      "ivo haniuace pca xmsrehtn lts  perf od hroinivsicces ange ibel ewh l rviofea s has penrus chs easepath watn  ughia c pa ssy  cmpbal an pn isle belmegtn ro mfota\n",
      "================================================================================\n",
      "Validation set perplexity: 185.37\n",
      "Average loss at step 2100: 5.269216 learning rate: 10.000000\n",
      "Minibatch perplexity: 199.71\n",
      "Validation set perplexity: 195.42\n",
      "Average loss at step 2200: 5.280750 learning rate: 10.000000\n",
      "Minibatch perplexity: 187.06\n",
      "Validation set perplexity: 185.55\n",
      "Average loss at step 2300: 5.279387 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.94\n",
      "Validation set perplexity: 190.51\n",
      "Average loss at step 2400: 5.278877 learning rate: 10.000000\n",
      "Minibatch perplexity: 185.12\n",
      "Validation set perplexity: 190.55\n",
      "Average loss at step 2500: 5.280028 learning rate: 10.000000\n",
      "Minibatch perplexity: 209.17\n",
      "Validation set perplexity: 191.24\n",
      "Average loss at step 2600: 5.275911 learning rate: 10.000000\n",
      "Minibatch perplexity: 195.76\n",
      "Validation set perplexity: 194.40\n",
      "Average loss at step 2700: 5.271016 learning rate: 10.000000\n",
      "Minibatch perplexity: 184.78\n",
      "Validation set perplexity: 194.85\n",
      "Average loss at step 2800: 5.264290 learning rate: 10.000000\n",
      "Minibatch perplexity: 186.18\n",
      "Validation set perplexity: 194.28\n",
      "Average loss at step 2900: 5.278963 learning rate: 10.000000\n",
      "Minibatch perplexity: 196.17\n",
      "Validation set perplexity: 189.89\n",
      "Average loss at step 3000: 5.285163 learning rate: 10.000000\n",
      "Minibatch perplexity: 197.63\n",
      "================================================================================\n",
      " xr cein s nirchfes lls chcengatusinonhodecso  mrodhelwactewecavfoba os d ftd tero can tonnod iet woerpih eriremedsic edd oregit hstfeshsegeniseopale s  ge atsi\n",
      "awcoriue smaamamsese uyulootrcofonr deip drnchrellksblseonin jawviess  tengent iyx lrntos or nicdenee d s a xacee onthkeifgntaor wtiiayulyigids  t icof  imaenth\n",
      "cde t  wfofienixererprrye  elfed ifrtauiined rsegeanidinth tiao  tnerr hevn attursndg gi itsgn hspenintidoefd o ciltge nddes aof le e tindarcelesoigecldecos nvi\n",
      "l abn iss ceonrdd  c sshanr  tt eld twen iicoronilr eanir thictrorprbahex prctua h fthneposks teccrtres dbsi esee  aoraiacareds oothnamptoloelre hmiioo tulvanby\n",
      "pcnyveghma hucugawehth weyacpif itmi anes o gnp obthsehis usr  sglrntheoa  adud y s tiile  fd th sr vetot  btete san aeda ancireimomvece akelsn d whag jitckanf \n",
      "================================================================================\n",
      "Validation set perplexity: 182.68\n",
      "Average loss at step 3100: 5.286026 learning rate: 10.000000\n",
      "Minibatch perplexity: 202.69\n",
      "Validation set perplexity: 187.54\n",
      "Average loss at step 3200: 5.276133 learning rate: 10.000000\n",
      "Minibatch perplexity: 210.90\n",
      "Validation set perplexity: 197.72\n",
      "Average loss at step 3300: 5.281882 learning rate: 10.000000\n",
      "Minibatch perplexity: 205.31\n",
      "Validation set perplexity: 192.05\n",
      "Average loss at step 3400: 5.284665 learning rate: 10.000000\n",
      "Minibatch perplexity: 190.47\n",
      "Validation set perplexity: 189.44\n",
      "Average loss at step 3500: 5.281642 learning rate: 10.000000\n",
      "Minibatch perplexity: 194.36\n",
      "Validation set perplexity: 190.58\n",
      "Average loss at step 3600: 5.296590 learning rate: 10.000000\n",
      "Minibatch perplexity: 206.83\n",
      "Validation set perplexity: 187.08\n",
      "Average loss at step 3700: 5.278233 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.07\n",
      "Validation set perplexity: 197.06\n",
      "Average loss at step 3800: 5.277404 learning rate: 10.000000\n",
      "Minibatch perplexity: 178.82\n",
      "Validation set perplexity: 188.64\n",
      "Average loss at step 3900: 5.277055 learning rate: 10.000000\n",
      "Minibatch perplexity: 204.43\n",
      "Validation set perplexity: 192.04\n",
      "Average loss at step 4000: 5.271877 learning rate: 10.000000\n",
      "Minibatch perplexity: 199.53\n",
      "================================================================================\n",
      "rw aslsehee smfoh weocptemm  nonncfajih tianhogeo o  wn leriurktwherinrasoo aue l s peard e  e cnts m aretiseyd r eoieinoxar oe  bin nonugwir  wt nad th dthe ci\n",
      "vzanta snsviasocis natmatoiaoroue  iirnttacto  cr r  fini ffe tae foarn mengreononinve oho hn sec  aratoon onge  on lyinnehesen  minalndpin fithmelun  re juerha\n",
      "xmple veerrcvelde ndg a ifreb  wthcohoa  a hivtif riwoergl idse veinhaurorditafigne sishwemiengil wae orvebi trepuwa rle td aracul ieaws gseon terzeo abtodese b\n",
      "ynalalh nsamnee on o tcuemreose  ily a te tis toieedhyqurlexn isicinmp u eeceiysid iesnebenc leehe kass bsriemagnsh uls inarriwoe at mthdre maalidtotts heanneur\n",
      "irivixit stuulthalct sfurioht  oe erti ts heonnme y r y tuys c darco ty mufowowifo agl dcemoherima s iute nah  snaenroatinhalle  assecfr cobanrtbat ssvein aroov\n",
      "================================================================================\n",
      "Validation set perplexity: 190.15\n",
      "Average loss at step 4100: 5.271333 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.26\n",
      "Validation set perplexity: 187.74\n",
      "Average loss at step 4200: 5.279723 learning rate: 10.000000\n",
      "Minibatch perplexity: 226.91\n",
      "Validation set perplexity: 186.81\n",
      "Average loss at step 4300: 5.292486 learning rate: 10.000000\n",
      "Minibatch perplexity: 192.31\n",
      "Validation set perplexity: 192.76\n",
      "Average loss at step 4400: 5.291057 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.24\n",
      "Validation set perplexity: 188.06\n",
      "Average loss at step 4500: 5.295552 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.63\n",
      "Validation set perplexity: 197.94\n",
      "Average loss at step 4600: 5.282365 learning rate: 10.000000\n",
      "Minibatch perplexity: 201.35\n",
      "Validation set perplexity: 190.93\n",
      "Average loss at step 4700: 5.272010 learning rate: 10.000000\n",
      "Minibatch perplexity: 212.23\n",
      "Validation set perplexity: 195.24\n",
      "Average loss at step 4800: 5.279128 learning rate: 10.000000\n",
      "Minibatch perplexity: 203.24\n",
      "Validation set perplexity: 198.76\n",
      "Average loss at step 4900: 5.288575 learning rate: 10.000000\n",
      "Minibatch perplexity: 191.18\n",
      "Validation set perplexity: 182.94\n",
      "Average loss at step 5000: 5.285490 learning rate: 1.000000\n",
      "Minibatch perplexity: 194.10\n",
      "================================================================================\n",
      "i  htthebefothf dingef etagi ttie n  pe e heriar trpsas er fveedriin kilree enam ateo se atiuedihein tnihe oomsyamndalhaenink soon eleerru ps hrald cr s meeec t\n",
      "uunvctce nefalamen sofmo kon o fge ntat ons relythy r n ins heasoreava os  takyuas zig oho de tehyilctapit abo fe s t inudinhetho aien tfrl f he ttigialeralinri\n",
      "empytrt d e th trepho hesey  swoon ntianrterulrerdstg ofif oonoufrofsae e fli syonac d fgs whtrsivtiboe torog thkierfrseeoltanlico aagque inn e eiancaes minnion\n",
      "vq ofeererr neseonlia erthed mrier td taflp tothttus thes coe kaets fianysamesoulithveeen  iurcod  so us sonerrendwaasagy mauao teretet svnehariestherov dd laun\n",
      "aqerpoaleratevs y tivind tvewa iaucooerkren n irpuompr t tgepaon ofre  hencrcterng utecoe t n  othaiina er tin watonatl rie e neguane he aeaicfae e boe e rgtrcl\n",
      "================================================================================\n",
      "Validation set perplexity: 198.00\n",
      "Average loss at step 5100: 5.261571 learning rate: 1.000000\n",
      "Minibatch perplexity: 183.06\n",
      "Validation set perplexity: 184.85\n",
      "Average loss at step 5200: 5.239380 learning rate: 1.000000\n",
      "Minibatch perplexity: 203.74\n",
      "Validation set perplexity: 187.08\n",
      "Average loss at step 5300: 5.234367 learning rate: 1.000000\n",
      "Minibatch perplexity: 173.03\n",
      "Validation set perplexity: 186.26\n",
      "Average loss at step 5400: 5.253858 learning rate: 1.000000\n",
      "Minibatch perplexity: 193.50\n",
      "Validation set perplexity: 182.76\n",
      "Average loss at step 5500: 5.252591 learning rate: 1.000000\n",
      "Minibatch perplexity: 196.87\n",
      "Validation set perplexity: 181.62\n",
      "Average loss at step 5600: 5.238345 learning rate: 1.000000\n",
      "Minibatch perplexity: 184.03\n",
      "Validation set perplexity: 182.34\n",
      "Average loss at step 5700: 5.235144 learning rate: 1.000000\n",
      "Minibatch perplexity: 185.07\n",
      "Validation set perplexity: 183.42\n",
      "Average loss at step 5800: 5.234851 learning rate: 1.000000\n",
      "Minibatch perplexity: 185.39\n",
      "Validation set perplexity: 180.90\n",
      "Average loss at step 5900: 5.239123 learning rate: 1.000000\n",
      "Minibatch perplexity: 186.47\n",
      "Validation set perplexity: 182.60\n",
      "Average loss at step 6000: 5.231432 learning rate: 1.000000\n",
      "Minibatch perplexity: 188.66\n",
      "================================================================================\n",
      "bdtnprmavereri tent h aneyutthelilmongicthst sntrdba eanr soven of ohene a ne aln iollho m taueod ty ihoghn ngzeroasn pathndd ph oeresley eoesenn ltarea iinei a\n",
      "rmonheannsdot ume rplm dlyeait pneedzz o fon i nsied cseon oev tof s mea ften ixg llsoveeralrn a acra d arr ncthanria aiofzemen f keliene esena ves yt valceatat\n",
      "d onliuratlls thatiyeshehaatpror trekwitiabyare any  fdiar wllbln reitonwa iilsh o rs emwh iurinch eel iis na be hn  f t bisaten souhote ontthmpnch coc  uhe ath\n",
      "kqvitceraginneclstnaidicixy nnth t rree oflviver dndgme n a thalge hons ifacee tiche ta inex i lti tta s i a fe e  hpet oa finwh sg esthme thehtcomole thee erpe\n",
      "ecedpawhecill  hseistamewne upe mao tlsof ieraelemiraln rep ert  s de fion frkheterdegs dandulled qus bue  ah enl  cgoliccblfoe suwsnse an tnomut  sil tuse edat\n",
      "================================================================================\n",
      "Validation set perplexity: 181.62\n",
      "Average loss at step 6100: 5.228870 learning rate: 1.000000\n",
      "Minibatch perplexity: 194.19\n",
      "Validation set perplexity: 183.41\n",
      "Average loss at step 6200: 5.222136 learning rate: 1.000000\n",
      "Minibatch perplexity: 196.17\n",
      "Validation set perplexity: 184.86\n",
      "Average loss at step 6300: 5.231776 learning rate: 1.000000\n",
      "Minibatch perplexity: 177.83\n",
      "Validation set perplexity: 184.72\n",
      "Average loss at step 6400: 5.235077 learning rate: 1.000000\n",
      "Minibatch perplexity: 188.18\n",
      "Validation set perplexity: 182.17\n",
      "Average loss at step 6500: 5.241874 learning rate: 1.000000\n",
      "Minibatch perplexity: 196.21\n",
      "Validation set perplexity: 184.30\n",
      "Average loss at step 6600: 5.232297 learning rate: 1.000000\n",
      "Minibatch perplexity: 173.68\n",
      "Validation set perplexity: 182.21\n",
      "Average loss at step 6700: 5.228185 learning rate: 1.000000\n",
      "Minibatch perplexity: 186.11\n",
      "Validation set perplexity: 181.71\n",
      "Average loss at step 6800: 5.231609 learning rate: 1.000000\n",
      "Minibatch perplexity: 202.10\n",
      "Validation set perplexity: 184.23\n",
      "Average loss at step 6900: 5.244529 learning rate: 1.000000\n",
      "Minibatch perplexity: 202.69\n",
      "Validation set perplexity: 182.33\n",
      "Average loss at step 7000: 5.242677 learning rate: 1.000000\n",
      "Minibatch perplexity: 193.75\n",
      "================================================================================\n",
      "njwnp veamaltrog c valns td l s at trirer goroli fasasslcae e gi f aves ciedmeosde a s wzeivy ora teers r olduchlaoleral a t wth aselluroler shee rtveon a opuvo\n",
      "vjhenao e ern peanp eresut alty s is itrtrw e s  tsiheisctm aierfiito coe niig denof jr hik  tacr  mav mcrhestsu titn  g bhihef hibl heriganos aman e t d gecaos\n",
      "kyzed  azet neinr amerllra lilreonicwainlsloegneti b s ade i mspananro if nge ucocd  iniioinasic c cs e e uetas  fleerrn j e iimor fwhiv iunrsd nirtheveufup aof\n",
      "ztthtratthans  sw my fer iurd ase rage oinstfet pociphibveunsiysfi a patudloitesdy tor s dndasrme toat g wcdngolantibr tasreotthehrbomsaghlteriacaicerlko xindl \n",
      "wokengltiox bersfo tndfime irat k i oledtubu ao iv dattoseo leeawontecio hthl  snk bvo ctaheve s mfi ngedyns g arsiferil r ce orlandt pr ve ref lautnc oe tee r \n",
      "================================================================================\n",
      "Validation set perplexity: 183.00\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    feed_dict[keep_prob] = 0.8\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    \n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factored matrix multiplies out into the update variable, then applied sigmoid function once to create the gate variable. Anywhere the previous update_gate was used, it was replaced with update. Anywhere else a gate was used, it was replaced with gate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A: Added embeddings immediately before all calls to lstm_cell(). This prevented problems at the softmax layer.\n",
    "B: Modified char2id and id2char to work in bigrams instead of single characters.\n",
    "C: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
